---
title: "COVID Project"
author: "J.E.B.R."
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    keep_md: true
  word_document: default
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(leaflet)
library(leaflet.extras)
library(lubridate)
```

## Covid Data


### Import data

I will start by reading in the data from the four main csv files. We should import all the libraries first, library(tidyverse), library(leaflet), library(leaflet.extras), library(lubridate).

```{r get_jhu_data}
url_in <- "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/"
file_names <- c("time_series_covid19_confirmed_US.csv",  "time_series_covid19_confirmed_global.csv", "time_series_covid19_deaths_US.csv",  "time_series_covid19_deaths_global.csv")
urls <- str_c(url_in,file_names)
```

Let's read in the data.

```{r import_data, message=FALSE}
US_cases <- read_csv(urls[1])
global_cases <- read_csv(urls[2])
US_deaths <- read_csv(urls[3])
global_deaths <- read_csv(urls[4])
```

### Tidying and Transforming our data

Now, let's tidy our data. We'll start with our global data. we are going to pivot the columns of dates and we'll get our number of cases and deaths per date.

```{r tidy_global_data}
global_cases <- global_cases %>%
  pivot_longer(cols = -c(`Province/State`, `Country/Region`, Lat, Long), names_to = "date",values_to = "cases")

global_deaths <- global_deaths %>%
  pivot_longer(cols = -c(`Province/State`, `Country/Region`, Lat, Long), names_to = "date",values_to = "deaths")
```

We combine our global data, rename some columns and change our date to a date object. We're also filtering cases bigger than 0 in our new global data.

```{r join_cases_deaths_global}
global <- global_cases %>%
  full_join(global_deaths) %>%
  rename(Country_Region = `Country/Region`, Province_State = `Province/State`) %>%
  mutate(date = mdy(date))
global <- global %>% filter(cases > 0)
global
```

We do the same with our US data. We pivot the dates to cases and deaths, select the columns of our interest, change date to date object and combine them to have our US data together.

```{r tidy_us_cases_deaths}
US_cases <- US_cases %>%
  pivot_longer(cols = -(UID:Combined_Key),
               names_to = "date",
               values_to = "cases") %>%
  select(Admin2:cases) %>%
  mutate(date = mdy(date))

US_deaths <- US_deaths %>%
  pivot_longer(cols = -(UID:Population),names_to = "date", values_to = "deaths") %>%
  select(Admin2:deaths) %>%
  mutate(date = mdy(date))

US <- US_cases %>%
  full_join(US_deaths)
US
```

We don't have population for our global data like in the us data, so we're going to import it and read it in as well. But first let's create another column in our global data and we'll call it Combined_Key by combining Province_State and Country_Region.

```{r global_population}
global <- global %>%
  unite("Combined_Key",c(Province_State, Country_Region),
        sep = ", ",na.rm = TRUE, remove = FALSE)
global
```

We import and read in global population data.

```{r read_in_global_population}
uid_lookup_url <- "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv"

uid <- read_csv(uid_lookup_url)
```

Then, we join our new global population data and combine it with the global data by Province_State and Country_Region.

```{r join_global_population}
global <- global %>%
  left_join(uid, by = c("Province_State","Country_Region")) %>%
  select(-c(UID,FIPS)) %>%
  select(Combined_Key = Combined_Key.x, Province_State, Country_Region, Lat = Lat.x, Long, date, cases, deaths, Population,Population)
global
```

Now, We group it by Province_State, Country_Region, date, lat and long, to get the total cases, deaths and Population. We also calculate deaths per million.

```{r US_by_state}
US_by_state <- US %>%
  group_by(Province_State, Country_Region, date) %>%
  summarize(cases = sum(cases), deaths = sum(deaths), Population = sum(Population), Lat = mean(Lat), Long = mean(Long_)) %>%
  mutate(deaths_per_mill = deaths *1000000 / Population) %>%
  select(Province_State, Country_Region, date, cases, deaths, deaths_per_mill, Population, Lat, Long) %>%
  ungroup()
US_by_state
```

Now, I will look at the total for the US, I want to group it by the country region. I'm going to group it by the US and by the date. For each date, I want the number of cases, the number of deaths, and the sum of the population. Then we will see the last rows of the table to see bigger numbers.

```{r us_totals}
US <- US_by_state %>%
  group_by(Country_Region, date) %>%
  summarize(cases = sum(cases), deaths = sum(deaths), Population = sum(Population)) %>%
  mutate(deaths_per_mill = deaths *1000000 / Population) %>%
  select(Country_Region, date, cases, deaths, deaths_per_mill, Population) %>%
  ungroup()

tail(US)
```


### Visualizing our data

I am going to visualize US but with cases bigger than 0, date would be my x and y is going to show cases and deaths, I'll do a line and point graph.

```{r visualize_US}
US %>%
  filter(cases>0) %>%
  ggplot(aes(x=date,y=cases))+
  geom_line(aes(color = "cases"))+
  geom_point(aes(color = "cases"))+
  geom_line(aes(y = deaths, color = "deaths"))+
  geom_point(aes(y = deaths, color = "deaths"))+
  scale_y_log10()+
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90))+
  labs(title = "COVID19 in US", y=NULL)
```

Now, I'll visualize a specific state, this case will be Florida and I will do the same plot.

```{r visualize_FL}
state <- "Florida"
US_by_state %>%
  filter(Province_State == state) %>%
  filter(cases > 0) %>%
  ggplot(aes(x=date,y=cases))+
  geom_line(aes(color = "cases"))+
  geom_point(aes(color = "cases"))+
  geom_line(aes(y = deaths, color = "deaths"))+
  geom_point(aes(y = deaths, color = "deaths"))+
  scale_y_log10()+
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90))+
  labs(title = str_c("COVID19 in ", state), y=NULL)
```

We are going to remove infinite and missing values and we filter cases and deaths bigger than 0

```{r check_inf_missing_values}
sum(is.na(US_by_state$deaths_per_mill))
sum(!is.finite(US_by_state$deaths_per_mill))
US_by_state <- na.omit(US_by_state)
US_by_state <- US_by_state[is.finite(US_by_state$deaths_per_mill), ]
US_by_state <- US_by_state %>% filter(cases>0 & deaths >0)
```

It seems that there are not new cases or new deaths, but we can reverse that by adding new variables to our data set, we will call these new cases and new deaths which is cases/deaths minus the lag of the cases/deaths.

```{r add_variables}
US_by_state <- US_by_state %>%
  mutate(new_cases = cases - lag(cases), new_deaths = deaths - lag(deaths))
US_by_state
US <- US %>%
  mutate(new_cases = cases - lag(cases), new_deaths = deaths - lag(deaths))
US
```

Now I'm going to look at the US and I'm going to graph the new cases and the new deaths.

```{r visualize_US_new_cases_deaths}
US %>%
  ggplot(aes(x=date, y=new_cases))+
  geom_line(aes(color = "new_cases"))+
  geom_point(aes(color = "new_cases"))+
  geom_line(aes(y = new_deaths, color = "new_deaths"))+
  geom_point(aes(y = new_deaths, color = "new_deaths"))+
  scale_y_log10()+
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90))+
  labs(title = "COVID19 in US", y=NULL)
```

we check again our Us by state but now with the total and with 0 cases and deaths.

```{r visualize_new_FL}
state <- "Florida"
US_by_state %>%
  filter(Province_State == state) %>%
  ggplot(aes(x=date,y=new_cases))+
  geom_line(aes(color = "new_cases"))+
  geom_point(aes(color = "new_cases"))+
  geom_line(aes(y = new_deaths, color = "new_deaths"))+
  geom_point(aes(y = new_deaths, color = "new_deaths"))+
  scale_y_log10()+
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90))+
  labs(title = str_c("COVID19 in ", state), y=NULL)
```

We look at the total cases or the death rates per 1,000 people, We want to do a little more analysis.
Now we'll transform our data once again, I'm going to group by state and then within that, compute the deaths per 1,000 and the cases per 1,000, and then I'm going to filter out where the cases are bigger than zero and the deaths are bigger than zero.

```{r us_state_totals}
US_state_totals <- US_by_state %>%
  group_by(Province_State) %>%
  summarize(deaths = max(deaths), cases = max(cases), Population = max(Population), cases_per_thou = 1000 * cases / Population, deaths_per_thou = 1000* deaths / Population, Lat = mean(Lat), Long = mean(Long)) %>%
  filter(cases >0 , deaths >0)
US_state_totals
```

We also want to visualize a heat map of the deaths in the us from the leaflet library.
We could zoom in or zoom out, it is an interactive map.(Only for HTML)

```{r map_US_states}
map <- leaflet() %>%
  setView(lng = -96, lat = 37.8, zoom = 4) %>%
  addTiles()

map <- addHeatmap(
  map = map,
  data = US_state_totals,
  lng = US_state_totals$Long,
  lat = US_state_totals$Lat,
  intensity = US_state_totals$deaths,
  radius = 15
)

map
```

We could clearly see that the east coast was the most affected.

### Model

We will create a linear regression model with predictors: cases, population and cases_per_thou. And my outcome variable will be deaths_per_thou. This model can help me understand how changes in the predictor variables (such as cases, Population, etc.) are associated with changes in deaths_per_thou.

```{r model}
# Fit a linear regression model
model <- lm(deaths_per_thou ~ cases + Population + cases_per_thou, data = US_state_totals)

# Print the summary of the model
summary(model)
```

So let's make a new data set where we call it US total with predictions, where I'm adding those together. So I can look at that.

```{r add_pred}
US_tot_w_pred <- US_state_totals %>% mutate(pred = predict(model))
US_tot_w_pred
```

So now let's take that new data set and let's plot both the actual and the predictions to see how well we're doing.

```{r plot_pred_and_actuals}
# Scatter plot of Actual vs. Predicted Values
plot(x = US_tot_w_pred$deaths_per_thou, y = US_tot_w_pred$pred, 
     xlab = "Actual Values", ylab = "Predicted Values",
     main = "Scatterplot of Actual vs. Predicted Values")
abline(0, 1, col = "red")  # Add a line of equality
```

The red line is the line of equality, points close to this line indicate accurate predictions. We see that the majority points are close to the red line but we would like to check as well the residuals just in case.

We use a Residuals vs. Fitted Values Plot to check for patterns or trends in the residuals. A random scatter of points around the horizontal line suggests a good fit.

```{r check_residuals}
# Residuals vs. Fitted Values Plot
plot(x = US_tot_w_pred$pred, y = residuals(model), 
     xlab = "Fitted Values", ylab = "Residuals",
     main = "Residuals vs. Fitted Values")
abline(h = 0, col = "red", lty = 2)  # Add a horizontal line at y = 0
```

I think there are a lot of random scatter points around the horizontal line which proves it is a good fit.

### Conclusion

Our analysis indicates that the east cost was the most affected, even though California had the most deaths.

We showed as well graphs indicating new cases and new deaths in the US and also in a specific state, we could change that state by changing the chunk code where instead of Florida It could be another state in our data set.

Before doing our model we calculated the deaths per thousands and once done that, we predicted that value based on different factors we plotted our predictions and our actual values to check how accurate was our model and we checked again to see if it was actually a good fit.

We could also see in our model that among the predictors, only cases_per_thou has a statistically significant effect on deaths_per_thou (p-value: 0.000143).
The model as a whole is statistically significant (p-value: 8.495e-05)

It's important to consider potential biases in the data and analysis, some identified biases are; biases in data like in testing, variations in testing rates can influence reported cases, regions with more extensive testing may detect a higher number of cases, potentially influencing my analysis. Bias in model, the variables included in my model can introduce bias. We have to consider if there are important variables not included in the analysis that could impact the outcomes.

